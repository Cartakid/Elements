# -*- coding: utf-8 -*-
"""Lab10 Juan Jose- EoML.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GIKbtGE9V2_m9EkY03FaIWb1OfR7-367

# Laboratorio 10

En este laboratorio encontraremos el valor óptimo de k en k-mean clustering con una gráfica de distancia cuadrada media respecto a k
"""

import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans

import matplotlib.pyplot as plt

print("Todos los paquetes han sido importados:")

"""Trabajaremos sobre la base de datos de casas de Wholesales costumers, que se encuentra en [Machine Learning Repository](http://archive.ics.uci.edu/ml/datasets/Wholesale+customers).

1.   Enumera qué variables son continuas y qué variables son categóricas
2.   Despliega la estadística descriptiva de las variables continuas del dataset para explorarlo
3.   Convierte las variables categóricas en binarias usando pd.get_dummies.
4.   Estandariza las variables usando una escala estándar (StandardScaler). Explica por qué este paso es importante.
5.   En un rango de 1 a 15 para k, realiza el clustering sobre los datos y para cada k guarda el valor de la distancia cuadrada media.
6.   Haz una gráfica de k contra la distancia cuadrada media. ¿Qué criterio puedes usar para identificar el valor óptimo de k? ¿Qué pasa cuando k se aproxima a n, la cantidad de observaciones?
7.   Repite el clustering ahora usando el valor óptimo de k. Ubica la posición de cada centroide y comenta acerca de cada uno (por ejemplo, ¿qué valores para cada variable caracterizan a cada centroide?)

#Upload File
"""

from google.colab import files
uploaded = files.upload()
import io
data= pd.read_csv(io.BytesIO(uploaded['Wholesale customers data.csv']))

"""#1Enumera las columnas continuas"""

cols = data.columns

num_cols = data._get_numeric_data().columns

list(set(cols) - set(num_cols))

"""#2 Estadística descriptiva"""

data.describe()

"""#3 Convertir variables categóricas en binarias.

No hay entonces no lo hice.

#4 Estandarizar las variables
"""

scaler = StandardScaler()
scaled_df = scaler.fit_transform(data)

#Es importante para estandarizar la data que la media=0 y la desviación=1 para aplicar ML

"""#5 k-mean cluster"""

X = data.iloc[:,3:8].values
K = 15
m=X.shape[0] 
n=X.shape[1] 
n_iter=100
Centroids=np.array([]).reshape(n,0) 
for i in range(K):
    rand=np.random.randint(0,m-1)
    Centroids=np.c_[Centroids,X[rand]]

EuclidianDistance=np.array([]).reshape(m,0)
for k in range(K):
       tempDist=np.sum((X-Centroids[:,k])**2,axis=1)
       EuclidianDistance=np.c_[EuclidianDistance,tempDist]
C=np.argmin(EuclidianDistance,axis=1)+1

Y={}
for k in range(K):
    Y[k+1]=np.array([]).reshape(2,0)
    
for i in range(m):
   Y[C[i]]=np.c_[Y[C[i]],X[i]]
     
for k in range(K):
    Y[k+1]=Y[k+1].T
    
for k in range(K):
     Centroids[:,k]=np.mean(Y[k+1],axis=0)

for i in range(n_iter):
     EuclidianDistance=np.array([]).reshape(m,0)
     for k in range(K):
          tempDist=np.sum((X-Centroids[:,k])**2,axis=1)
          EuclidianDistance=np.c_[EuclidianDistance,tempDist]
      C=np.argmin(EuclidianDistance,axis=1)+1
      Y={}
      for k in range(K):
          Y[k+1]=np.array([]).reshape(2,0)
      for i in range(m):
          Y[C[i]]=np.c_[Y[C[i]],X[i]]
     
      for k in range(K):
          Y[k+1]=Y[k+1].T
    
      for k in range(K):
          Centroids[:,k]=np.mean(Y[k+1],axis=0)
      Output=Y

"""#6 Graph"""

color=['red','blue','green','cyan','magenta']
labels=['cluster1','cluster2','cluster3','cluster4','cluster5']
for k in range(K):
    plt.scatter(Output[k+1][:,0],Output[k+1][:,1],c=color[k],label=labels[k])
plt.scatter(Centroids[0,:],Centroids[1,:],s=300,c='yellow',label='Centroids')
plt.xlabel('Income')
plt.ylabel('Number of transactions')
plt.legend()
plt.show()

"""#7 Valor optimo de k"""