# -*- coding: utf-8 -*-
"""Laboratorio 5 Juan Jose - EoML.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Vse24ZPC1ukDXiHF02d_FktIAGce2_kB

# Laboatorio 5

En este laboratorio aprenderemos a aplicar validación cruzada de k-folios.
"""

import numpy as np
import pandas as pd

import matplotlib
import matplotlib.pyplot as plt

import seaborn as sns

import statsmodels
import statsmodels.formula.api as smf

from sklearn.model_selection import KFold

print("Todos los paquetes han sido importados:")

"""## Validación cruzada

Es un procedimiento que evalúa las habilidades predictivas de un modelo en una muestra limitada. El procedimiento general es

1.   Reordenar la muestra aleatoriamente
2.   Separar el dataset en k grupos
3.   Tomar k-1 grupos como datos de entrenamiento y el otro como grupo de validación
4.   Entrenar un modelo sobre uno de los grupos de entrenamiento
5.   Evaluar el modelo sobre el grupo restante correspondiente, el grupo de validación
6.   Guardar alguna medida de habilidad del modelo y descartar el modelo
7.   Repetir 4 a 6 tomando un nuevo split

Cualquier preparación de los datos debe aplicarse entre los pasos 2 y 3, no antes. De lo contrario, se estará sobreestimando las habilidades de los modelos (como descrito en la sección 7.10.2 del libro "The elements of machine learning")

### Cómo separar un grupo de datos

Imagina que tenemos una muestra de 6 observaciones
"""

data = np.array([1,2,3,4,5,6])
kfold = KFold(3,True,1) #Recuerda que puedes revisar la documentación de nuevas funciones usando Tab luego del primer paréntesis
splitdata = kfold.split(data) # Esta función genera listas de índices separadas como indicado
for train, test in splitdata:
  # imprimo en la línea 'split' los índices train y test que generó kfold.split
  print('split %s, %s\ntrain: %s, test: %s' % (train,test,data[train], data[test]))

"""kfold.split separó los datos como esperado. Cada una de las observaciones aparece k-1 veces como parte de un grupo de entrenamiento y 1 vez como parte del grupo de validación. Para aplicar esto a observaciones con un target, simplemente lo colocamos también en el argumento de kfold.split:"""

data = np.array(([1,2,3,4,5,6],[10,20,30,40,50,60]))
kfold = KFold(3,True,1)
splitdata = kfold.split(X = data[0], y = data[1])
for train, test in splitdata:
  print('split %s, %s\ntrain x: %s, test x: %s' % (train,test,data[0][train], data[0][test]))
  print('train y: %s, test y: %s' % (data[1][train], data[1][test]))

"""### Aplicación completa del procedimiento CV
Ahora, para cada split entremos algún modelo y evaluamos en su grupo respectivo. Usaré una regresión logística (no muy astuto, eh?)
"""

data = np.array(([1,2,3,4,5,6],[0.1,0.2,0.3,0.4,0.5,0.6]))
# Pasos 1 y 2 ocurren en la siguiente línea
kfold = KFold(3,True,1)
# Paso 3 ocurre al usar split
splitdata = kfold.split(X = data[0], y = data[1])
RMSE_i = [] # Aquí es donde guardaré mi medida de varianza para cada modelo
for train, test in splitdata:
  # 4. Ajustar una regresión logística en un grupo de entrenamiento
  model = smf.Logit(data[1][train], data[0][train]).fit()
  # 5. Evaluar en el grupo de entrenamiento
  y_hat = model.predict(data[0][test])
  # 6. Guardar una medida de habilidad del modelo
  RMSE_i.append((sum((y_hat - data[1][test])**2)/len(y_hat))**(1/2))
  # Al repetir el loop, descartamos el modelo y repetimos los pasos para otro split
  
RMSE = np.mean(RMSE_i)
print(RMSE)

"""### Ejercicio

Haz el mismo procedimiento que hicimos con la regresión logística, solo que ahora con una regresión lineal. Compara los RMSE
"""

data = np.array(([1,2,3,4,5,6],[0.1,0.2,0.3,0.4,0.5,0.6]))
# Pasos 1 y 2 ocurren en la siguiente línea
kfold = KFold(3,True,1)
# Paso 3 ocurre al usar split
splitdata = kfold.split(X = data[0], y = data[1])
RMSE_i = [] # Aquí es donde guardaré mi medida de varianza para cada modelo
for train, test in splitdata:
  # 4. Ajustar una regresión logística en un grupo de entrenamiento
  model = smf.OLS(data[1][train], data[0][train]).fit()
  # 5. Evaluar en el grupo de entrenamiento
  y_hat = model.predict(data[0][test])
  # 6. Guardar una medida de habilidad del modelo
  RMSE_i.append((sum((y_hat - data[1][test])**2)/len(y_hat))**(1/2))
  # Al repetir el loop, descartamos el modelo y repetimos los pasos para otro split
  
RMSE = np.mean(RMSE_i)
print(RMSE)

"""## Ejercicio de aplicación

Utiliza los datos de Iris (los del laboratorio 2) para investigar la dependencia de la varianza () con el valor k al hacer el procedimiento de validación cruzada. Como resultado, debes presentar una gráfica de RMSE contra k de por lo menos 10 diferentes valores de k.
"""

from sklearn.datasets import load_iris
from sklearn.neighbors import KNeighborsClassifier

iris = load_iris()
ir = pd.DataFrame(iris.data, columns=iris.feature_names)
fl = pd.DataFrame(iris.target, columns=['flowers'])

RMSE_Ks = []
n_neighbors = 3
knn = KNeighborsClassifier(n_neighbors)

def crossFunc(k):

  kfold = KFold(k,True,1)
  splitdata = kfold.split(X = iris.data, y = iris.target)
  RMSE_i = []

  for train, test in splitdata:
    # 4. Ajustar knn en un grupo de entrenamiento
    model = knn.fit(iris.data[train], iris.target[train])
  
    # 5. Evaluar en el grupo de entrenamiento
    y_pred= model.predict(iris.data[test])
  
    # 6. Guardar una medida de habilidad del modelo
    RMSE_i.append((sum((y_pred - iris.target[test])**2)/len(y_pred))**(1/2))
  
  RMSE = np.mean(RMSE_i)
  print("RMSE: ",RMSE)


  RMSE_Ks.append(RMSE)

crossFunc(3)
crossFunc(6)
crossFunc(9)
crossFunc(12)
crossFunc(15)
crossFunc(18)
crossFunc(21)
crossFunc(24)
crossFunc(27)
crossFunc(30)

"""**Grafica RMSE para K**"""

import matplotlib
import matplotlib.pyplot as plt
ks = np.array([3,6,9,12,15,18,21,24,27,30])

plt.plot(ks, RMSE_Ks);
plt.xlabel("no. K")
plt.ylabel("RMSE")