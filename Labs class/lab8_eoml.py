# -*- coding: utf-8 -*-
"""Lab 8 Juan Jose - EoML.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12A7FJ1lgm4piBOZMFcrimhrRXVIaLJUz

# Laboratorio 8

En este laboratorio aplicaremos regresión polinomial de grado n para estudiar el sobreajuste a datos generados con ruido aleatorio
"""

import numpy as np
import pandas as pd

import matplotlib
import matplotlib.pyplot as plt

import seaborn as sns

import statsmodels
import statsmodels.formula.api as smf

from sklearn.datasets import load_boston

print("Todos los paquetes han sido importados:")

"""Generemos datos a partir de una función senoidal"""

import random
from matplotlib.pylab import rcParams

#Tomemos ángulos de 0 a 300
x = np.array([i*np.pi/180 for i in range(0,300,4)])
np.random.seed(10)  #El error aleatorio será reproducible
y = np.sin(x) + np.random.normal(0,0.15,len(x))
data = pd.DataFrame(np.column_stack([x,y]),columns=['x','y'])
plt.plot(data['x'],data['y'],'.')

"""## Ejercicios

1.   Sobre los datos generados, realiza regresiones polinomiales de grado 1 a 16. Guarda los coeficientes, RMS de entrenamiento y R^2 para cada regresión
2.   Genera un nuevo set de datos usando a partir de la misma función seno y utiliza este conjunto como el set de prueba. Guarda el RMS de prueba para cada uno.
3.   Compara los RMS de prueba de todas las regresiones. ¿Hay algún grado polinomial que minimiza el RMS de prueba para este modelo? ¿Notas sobreajuste para polinomios de alto grado?
4.   Repite este procedimiento aumentando la magnitud de los errores aleatorios tres veces.
"""

def poly(x, degree = 1):
    n = degree + 1
    x = np.asarray(x).flatten()
    if(degree >= len(np.unique(x))):
        print("'degree' must be less than number of unique points")
        return
    xbar = np.mean(x)
    x = x - xbar
    X = np.fliplr(np.vander(x, n))
    q, r = np.linalg.qr(X)

    z = np.diag(np.diag(r))
    raw = np.dot(q, z)
    
    norm2 = np.sum(raw ** 2, axis=0)
    alpha = (np.sum((raw ** 2) * np.reshape(x,(-1,1)), axis=0) / norm2 + xbar)[:degree]
    Z = raw / np.sqrt(norm2)
    return Z, norm2, alpha

X = poly(data["x"].values, 4)[0]
X[0:5, 1:]

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
x1 = data["x"].values
x2 = np.power(x1, 2)
x3 = np.power(x1, 3)
x4 = np.power(x1, 4)
X = np.vstack((x1, x2, x3, x4)).T
X[0:5, :]
X = np.vstack((x1, x2, x3)).T
y = data["x"].map(lambda w: 1 if w > 250 else 0).values
reg = LinearRegression()
reg.fit(X, y)
print ("Intercepts:", reg.intercept_)
print ("Coefficients:", reg.coef_)
ypred = reg.predict(X)
print ("MSE:", mean_squared_error(y, ypred))

# Plot the predicted probability of being a high-earner over age range
bs = data["x"].values
xs = range(np.min(bs), np.max(bs))
ys = [reg.predict_proba(np.array([x, x*x, x*x*x]))[0][0] for x in xs]
plt.plot(xs, ys)
plt.xlabel("age")
plt.ylabel("p(x > 250k)")