# -*- coding: utf-8 -*-
"""Lab 9 Juan Jose - EoML.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xMEjG1Jejk2-lg8u4JuIspUjI5ErxXxO

# Laboratorio 9

En este laboratorio usaremos boosting sobre árboles de decisión y exploraremos cómo optimizar el modelo respecto a los parámetros de boosting (número de árboles, rapidez de aprendizaje y profundidad)
"""

import numpy as np
import pandas as pd

import matplotlib as mpl
import matplotlib.pyplot as plt

import seaborn as sns

import statsmodels
import statsmodels.formula.api as smf

from sklearn.datasets import load_boston
from sklearn.ensemble import GradientBoostingClassifier

print("Todos los paquetes han sido importados:")

"""Trabajaremos sobre la base de datos de casas de Boston de skLearn. Para la implementación del GradientBoosting, puedes referirte a un ejemplo con datos del Titanic en [Medium](https://medium.com/all-things-ai/in-depth-parameter-tuning-for-gradient-boosting-3363992e9bae).

1.   Clasifica los precios de casas en categorías ordenadas. Esta nueva variable será tu target.
2.   Haz el Boosting usando valores para el *learning rate*: 1, 0.5, 0.25, 0.1, 0.05, 0.01. Utiliza alguna medida apropiada para medir la efectividad del modelo en cada caso y busca un valor óptimo. **Explica las ventajas y desventajas de un aprendizaje rápido contra uno lento.**
3.   Repite el Boosting usando valores para el *number of trees*: 1, 2, 4, 8, 16, 32, 64, 100, 200. Utiliza alguna medida apropiada para medir la efectividad del modelo en cada caso y busca un valor óptimo. **Explica las ventajas y desventajas de usar muchos o pocos árboles**
4.   Repite el Boosting usando valores para el *max_depth*: entre 1 y 32, enteros. Utiliza alguna medida apropiada para medir la efectividad del modelo en cada caso y busca un valor óptimo. **Explica las ventajas y desventajas de usar una profundidad muy alta o muy baja**
"""

data=load_boston()
df = pd.DataFrame(data.data, columns=data.feature_names)

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(df, y, test_size=0.25)

model = GradientBoostingClassifier()
model.fit(x_train, y_train)
GradientBoostingClassifier(criterion=friedman_mse, init=None,
 learning_rate=0.1, loss=deviance, max_depth=3,
 max_features=None, max_leaf_nodes=None,
 min_impurity_split=1e-07, min_samples_leaf=1,
 min_samples_split=2, min_weight_fraction_leaf=0.0,
 n_estimators=100, presort=auto, random_state=None,
 subsample=1.0, verbose=0, warm_start=False)
y_pred = model.predict(x_test)

learning_rates = [1, 0.5, 0.25, 0.1, 0.05, 0.01]
train_results = []
test_results = []
for eta in learning_rates:
   model = GradientBoostingClassifier(learning_rate=eta)
   model.fit(x_train, y_train)
   train_pred = model.predict(x_train)
   false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, train_pred)
   roc_auc = auc(false_positive_rate, true_positive_rate)
   train_results.append(roc_auc)
   y_pred = model.predict(x_test)
   false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)
   roc_auc = auc(false_positive_rate, true_positive_rate)
   test_results.append(roc_auc)
from matplotlib.legend_handler import HandlerLine2D
line1, = plt.plot(learning_rates, train_results, b, label='Train AUC')
line2, = plt.plot(learning_rates, test_results, r, label='Test AUC')
plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})
plt.ylabel('AUC score')
plt.xlabel('learning rate')
plt.show()

n_estimators = [1, 2, 4, 8, 16, 32, 64, 100, 200]
train_results = []
test_results = []
for estimator in n_estimators:
   model = GradientBoostingClassifier(n_estimators=estimator)
   model.fit(x_train, y_train)
   train_pred = model.predict(x_train)
   false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, train_pred)
   roc_auc = auc(false_positive_rate, true_positive_rate)
   train_results.append(roc_auc)
   y_pred = model.predict(x_test)
   false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)
   roc_auc = auc(false_positive_rate, true_positive_rate)
   test_results.append(roc_auc)
from matplotlib.legend_handler import HandlerLine2D
line1, = plt.plot(n_estimators, train_results, b, label='Train AUC')
line2, = plt.plot(n_estimators, test_results, r, label='Test AUC')
plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})
plt.ylabel('AUC score')
plt.xlabel('n_estimators')
plt.show()

max_depths = np.linspace(1, 32, 32, endpoint=True)
train_results = []
test_results = []
for max_depth in max_depths:
   model = GradientBoostingClassifier(max_depth=max_depth)
   model.fit(x_train, y_train)
   train_pred = model.predict(x_train)
   false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, train_pred)
   roc_auc = auc(false_positive_rate, true_positive_rate)
   train_results.append(roc_auc)
   y_pred = model.predict(x_test)
   false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)
   roc_auc = auc(false_positive_rate, true_positive_rate)
   test_results.append(roc_auc)
from matplotlib.legend_handler import HandlerLine2D
line1, = plt.plot(max_depths, train_results, b, label='Train AUC')
line2, = plt.plot(max_depths, test_results, r, label='Test AUC')
plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})
plt.ylabel('AUC score')
plt.xlabel('Tree depth')
plt.show()