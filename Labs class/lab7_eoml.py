# -*- coding: utf-8 -*-
"""Laboratorio 7 Juan Jose- EoML.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1e0w3Kk57tX0a7qj_Hf39LvdpEZ4fP9FI
"""

#Import Linear Regression model from scikit-learn.
from sklearn.linear_model import LinearRegression
def linear_regression(data, power):
    #initialize predictors:
    predictors=['x']
    if power>=2:
        predictors.extend(['x_%d'%i for i in range(2,power+1)])
    
    #Fit the model
    linreg = LinearRegression(normalize=True)
    linreg.fit(data[predictors],data['y'])
    y_pred = linreg.predict(data[predictors])
    
    #Return the result in pre-defined format
    rss = sum((y_pred-data['y'])**2)
    ret = [rss]
    ret.extend([linreg.intercept_])
    ret.extend(linreg.coef_)
    return ret

"""# Laboratorio 7

En este laboratorio aplicaremos regresión polinomial de grado n para estudiar el sobreajuste a datos generados con ruido aleatorio
"""

import numpy as np
import pandas as pd

import matplotlib
import matplotlib.pyplot as plt

import seaborn as sns

import statsmodels
import statsmodels.formula.api as smf

from sklearn.datasets import load_boston

print("Todos los paquetes han sido importados:")

"""## Código para hacer las regresiones

Muestro un ejemplo de una función para hacer regresiones polinomiales. Modifica esta función para que también haga la gráfica a tu gusto de la regresión

Generemos datos a partir de una función senoidal
"""

boston=load_boston()
boston_df=pd.DataFrame(boston.data,columns=boston.feature_names)

# Agregamos la columna de 'Price' como target
boston_df['Price']=boston.target

# newX serán los predictores menos 'Price' y newY será el target, solo 'Price'
dataX = boston_df.drop('Price',axis=1)

# estandarizamos los datos en X
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
StandardScaler(copy=True, with_mean=True, with_std=True)

newX = pd.DataFrame(scaler.fit_transform(dataX),columns=list(dataX))

print(newX[0:3]) # check
newY=boston_df['Price']
print(newY[0:3])

"""No haremos ahora ningun tipo de validación o validación cruzada, así que usaremos directamente newX y newY para general el modelo"""

from sklearn.linear_model import Ridge

#En el argumento de Ridge escogemos el valor lambda (o alpha) de la regulariza-
  # ción, escojo 0.01, uno bajo para comenzar

ridgereg = Ridge(alpha=0.01)

ridgeModel = ridgereg.fit(newX,newY)
y_pred = ridgereg.predict(newX)

rss = sum((y_pred-newY)**2)
ret = [rss]
ret.extend([ridgereg.intercept_])
ret.extend(ridgereg.coef_)

# Esta gráfica muestra la proyección de los ejes 'CRIM' y 'PRICE'
plt.plot(newX['CRIM'],y_pred,'.',color = 'red')
plt.plot(newX['CRIM'],newY,'.')
plt.title('Plot for alpha: 0.01')

names = ['rss', 'intercept']
cols = list(newX)
print(cols)

for i in cols:
  names.append('coef '+i)

for i in range(0,len(ret)):
  print(names[i]+': '+str(ret[i]))

print(ret)

"""## Ejercicios

1.   Sobre los datos generados, realiza regresiones polinomiales de grado 1 a 16. Guarda los coeficientes, RMS de entrenamiento y R^2 para cada regresión
2.   Genera un nuevo set de datos usando a partir de la misma función seno y utiliza este conjunto como el set de prueba. Guarda el RMS de prueba para cada uno.
3.   Compara los RMS de prueba de todas las regresiones. Hay algún grado polinomial que minimiza el RMS de prueba para este modelo?
4.   Repite este procedimiento aumentando la magnitud de los errores aleatorios.
"""

from sklearn.preprocessing import PolynomialFeatures
from sklearn.model_selection import train_test_split
X = boston_df.drop('Price', axis=1)
y = pd.DataFrame(boston_df['Price'])
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)
poly_reg = PolynomialFeatures(degree = 2)
X_poly_train = poly_reg.fit_transform(pd.DataFrame(X_train['DIS']))
X_poly_test = poly_reg.fit_transform(pd.DataFrame(X_test['DIS']))
poly_result = poly_reg.fit(X_poly_train, y_train)

from sklearn.linear_model import LinearRegression
poly_model = LinearRegression()
poly_result = poly_model.fit(X_poly_train, y_train)
y_poly_pred = poly_model.predict(X_poly_test)

rss_train = sum((y_pred-newY)**2)
new=rss_train(poly_model.score(X_test['DIS'],y_test))
rsme_test=[rss_train]
rsme_test.append(np.sqrt(metrics.mean_squared_error(y_pred, y_test)))
coeff.append(poly_model.coef_)