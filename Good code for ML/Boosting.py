# -*- coding: utf-8 -*-
"""Boosting - master.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uPNumzpPwlD0vjm4eJ4i4kJ35McNfLbs

# Import packages
"""

import pandas as pd
import numpy as np
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

"""# Import data"""

# Import data from URL and add column names
auto_df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.data', header = None
                     , na_values = '?')

auto_df.columns = [
 'symboling'
 ,'normalized-losses'
 ,'make'
 ,'fuel-type'
 ,'aspiration'
 ,'num-of-doors'
 ,'body-style'
 ,'drive-wheels'
 ,'engine-location'
 ,'wheel-base'
 ,'length'
 ,'width'
 ,'height'
 ,'curb-weight'
 ,'engine-type'
 ,'num-of-cylinders'
 ,'engine-size'
 ,'fuel-system'
 ,'bore'
 ,'stroke'
 ,'compression-ratio'
 ,'horsepower'
 ,'peak-rpm'
 ,'city-mpg'
 ,'highway-mpg'
 , 'price'
]

# Check out the dataframe
auto_df.tail()

# Look at the columns we have available for prediction
auto_df.columns

"""# Data cleaning"""

# Take a subset of the columns
auto_sub = auto_df[['fuel-type', 'engine-location', 'engine-size', 'horsepower', 'peak-rpm', 'city-mpg']]

# See if there are NAs
auto_sub.isna().sum()

# Impute values for columns with NAs
auto_sub['horsepower'].fillna(auto_sub['horsepower'].median(), inplace=True)
auto_sub['peak-rpm'].fillna(auto_sub['peak-rpm'].median(), inplace=True)

# Check that imputation worked
auto_sub.isna().sum()

# Separate our response variable
y = auto_sub['city-mpg']
auto_sub.drop(axis=1, labels='city-mpg', inplace=True)

y.tail()

# Create a categorical response variable
yc = auto_df['make']

# Look at how many unique makes there are
yc.value_counts()

# Look at features
auto_sub.tail()

# Create df of object features and cast them as categorical
auto_category = auto_sub.select_dtypes(object).astype('category')

# Check that it worked
auto_category.dtypes

# Get dummies of your categorical variables
auto_category = pd.get_dummies(auto_category)

# Check out your dummies
auto_category.tail()

# Put the non-object variables in another df
auto_numeric = auto_sub.select_dtypes(exclude='object')

# Combine dummies and numeric variables back to one final feature df
auto_final = auto_category.join(auto_numeric)

# Make sure the df looks right
auto_final.tail()

"""# Train test split"""

X_train, X_test, y_train, y_test = train_test_split(auto_final, y, test_size=0.30, random_state=100)

Xc_train, Xc_test, yc_train, yc_test = train_test_split(auto_final, yc, test_size=0.30, random_state=100)

print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)

"""# Adaboost Regression"""

# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html
from sklearn.ensemble import AdaBoostClassifier

# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html
from sklearn.ensemble import AdaBoostRegressor

# Define model parameters
adaboost_reg = AdaBoostRegressor(n_estimators=100, learning_rate=1, loss='linear')

# Train model
adaboost_reg.fit(X_train, y_train)

# Find train and test predictions
prediction_train = adaboost_reg.score(X_train,y_train)
prediction_test = adaboost_reg.score(X_test,y_test)
print(prediction_train)
print(prediction_test)

"""# Adaboost Classification"""

# Define and train classification model
AdaBoost = AdaBoostClassifier(n_estimators=400,learning_rate=1,algorithm='SAMME')
AdaBoost.fit(Xc_train,yc_train)
c_pred_train = AdaBoost.score(Xc_train,yc_train)
c_pred_test = AdaBoost.score(Xc_test,y_test)

# Train and test accuracy
print(c_pred_train)
print(c_pred_test)

"""# XGBoost"""

! pip install xgboost

# https://xgboost.readthedocs.io/en/latest/python/python_intro.html
import xgboost as xgb

# Add data to XGBoost data matrices
dtrain = xgb.DMatrix(X_train, label=y_train)
dtest = xgb.DMatrix(X_test, label=y_test)

# Define model parameters
param = {'booster': 'gbtree', 'max_depth': 5, 'eta': 0.8, 'gamma': .01} #Classification objective:binary:logistic, 'objective': 'reg:squarederror' by default

# Train model
num_round = 100
bst = xgb.train(param, dtrain, num_round)

# Get model predictions
y_train_pred = bst.predict(dtrain)
y_test_pred = bst.predict(dtest)

# Find train and test r^2 values
print(r2_score(y_train, y_train_pred))
print(r2_score(y_test, y_test_pred))