# -*- coding: utf-8 -*-
"""Decision Tree - master.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PAe0woIOQDqlZlZxXiLQ6To5WxNlul7D

# Decision Trees

- Decision tree fitting
- Visualization
- Metrics
- Parameters
"""

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

from sklearn import datasets
from sklearn.model_selection import train_test_split

import seaborn as sns
from math import sqrt
# %matplotlib inline

"""## Import data"""

# Import data from URL and add column names
auto_df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.data', header = None
                     , na_values = '?')

auto_df.columns = [
 'symboling'
 ,'normalized-losses'
 ,'make'
 ,'fuel-type'
 ,'aspiration'
 ,'num-of-doors'
 ,'body-style'
 ,'drive-wheels'
 ,'engine-location'
 ,'wheel-base'
 ,'length'
 ,'width'
 ,'height'
 ,'curb-weight'
 ,'engine-type'
 ,'num-of-cylinders'
 ,'engine-size'
 ,'fuel-system'
 ,'bore'
 ,'stroke'
 ,'compression-ratio'
 ,'horsepower'
 ,'peak-rpm'
 ,'city-mpg'
 ,'highway-mpg'
 , 'price'
]

# Top 5 rows to get a sense of what the data looks like
auto_df.head()

germans = ['volkswagen', 'audi', 'bmw', 'mercedes-benz', 'porsche']

keep_features = [
 'fuel-type',
 'aspiration',
 'num-of-doors',
 'body-style',
 'drive-wheels',
 'curb-weight',
 'num-of-cylinders',
 'horsepower',
 'peak-rpm',
 'city-mpg',
 'highway-mpg',
 'price'
]

labels = auto_df[auto_df['make'].isin(germans)]['make']
features = auto_df[auto_df['make'].isin(germans)][keep_features]

features.shape, labels.shape

"""## initial viz"""

features.describe()

"""### Data cleaning"""

features.isna().sum()

from sklearn.impute import SimpleImputer

imputer = SimpleImputer(strategy='mean', missing_values=np.nan)

features['price']  = imputer.fit_transform(features[['price']])

features.shape

features.dtypes

from sklearn.preprocessing import LabelEncoder

feature_dummies = pd.get_dummies(features)

"""## Preparing our data"""

X_train, X_test, y_train, y_test = train_test_split(feature_dummies, labels, test_size=0.30, random_state=202)

X_train.shape

X_test.shape

y_test

"""# Decision Tree"""

from sklearn.tree import DecisionTreeClassifier, export_graphviz

clf = DecisionTreeClassifier()

clf = clf.fit(X_train, y_train)

"""### Visualization"""

! pip install graphviz

# Windows
# !set PATH=PATH;C:\path\to\anaconda\Library\bin\graphviz\

# Mac
# brew install graphviz

# Linux
# sudo apt-get install graphviz

import graphviz

dot_data = export_graphviz(clf, out_file=None, 
                     feature_names=feature_dummies.columns,  
                     class_names=labels.unique(),  
                     filled=True, rounded=True,  
                     special_characters=True)  
graph = graphviz.Source(dot_data)  
graph

"""## Metrics"""

from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
from sklearn.utils.multiclass import unique_labels

def plot_confusion_matrix(y_true, y_pred,
                          normalize=False,
                          title=None,
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    if not title:
        if normalize:
            title = 'Normalized confusion matrix'
        else:
            title = 'Confusion matrix, without normalization'

    # Compute confusion matrix
    cm = confusion_matrix(y_true, y_pred)
    # Only use the labels that appear in the data
    classes = unique_labels(y_true, y_pred)
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    print(cm)

    fig, ax = plt.subplots()
    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)
    ax.figure.colorbar(im, ax=ax)
    # We want to show all ticks...
    ax.set(xticks=np.arange(cm.shape[1]),
           yticks=np.arange(cm.shape[0]),
           # ... and label them with the respective list entries
           xticklabels=classes, yticklabels=classes,
           title=title,
           ylabel='True label',
           xlabel='Predicted label')

    # Rotate the tick labels and set their alignment.
    plt.setp(ax.get_xticklabels(), rotation=45, ha="right",
             rotation_mode="anchor")

    # Loop over data dimensions and create text annotations.
    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            ax.text(j, i, format(cm[i, j], fmt),
                    ha="center", va="center",
                    color="white" if cm[i, j] > thresh else "black")
    fig.tight_layout()
    return ax

y_pred = clf.predict(X_test)

y_pred

accuracy_score(y_pred, y_test)

print(classification_report(y_pred, y_test))

plot_confusion_matrix(y_true=y_test, y_pred=y_pred,
                      title='Confusion matrix')

[unique_labels(y_test, y_pred)]

datasets.load_iris().target_names

pd.DataFrame({'Actual': y_test,'Predictions': y_pred})

"""## Hyperparameters
- Max depth
- Max features
    - auto
    - sqrt
    - log2
    - int
    - float
- Min Samples split
- Min samples leaf
"""

clf = DecisionTreeClassifier(max_depth=2)

clf = clf.fit(X_train, y_train)

dot_data = export_graphviz(clf, out_file=None, 
                     feature_names=feature_dummies.columns,  
                     class_names=labels.unique(),  
                     filled=True, rounded=True,  
                     special_characters=True)  
graph = graphviz.Source(dot_data)  
graph

y_pred = clf.predict(X_test)

plot_confusion_matrix(y_true=y_test, y_pred=y_pred,
                      title='Confusion matrix')

