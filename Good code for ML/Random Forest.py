# -*- coding: utf-8 -*-
"""Random Forest - master.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cB7npyG665s9aUvUzvfNPNS88zvK7gkQ

# Random Forest

- Data exploration
- Random Forest fit
- Visualization and explore RandomForest
- Metrics
- Parameters
"""

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

from sklearn import datasets
from sklearn.model_selection import train_test_split

import seaborn as sns
from math import sqrt
# %matplotlib inline

"""## Import data"""

# Import data from URL and add column names
auto_df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.data', header = None
                     , na_values = '?')

auto_df.columns = [
 'symboling'
 ,'normalized-losses'
 ,'make'
 ,'fuel-type'
 ,'aspiration'
 ,'num-of-doors'
 ,'body-style'
 ,'drive-wheels'
 ,'engine-location'
 ,'wheel-base'
 ,'length'
 ,'width'
 ,'height'
 ,'curb-weight'
 ,'engine-type'
 ,'num-of-cylinders'
 ,'engine-size'
 ,'fuel-system'
 ,'bore'
 ,'stroke'
 ,'compression-ratio'
 ,'horsepower'
 ,'peak-rpm'
 ,'city-mpg'
 ,'highway-mpg'
 , 'price'
]

# Top 5 rows to get a sense of what the data looks like
auto_df.head()

auto_df.dropna(axis=0, subset=['price'], inplace=True)

labels = auto_df['price']
features = auto_df.drop('price', axis=1)

features.shape, labels.shape

"""### Data cleaning"""

features.isna().sum()

"""### Turn object types to categorical"""

features.dtypes

"""### categorical"""

category = features.select_dtypes(object).astype('category')

category.head()

category.isna().sum()

category['num-of-doors'] = category['num-of-doors'].fillna(category['num-of-doors'].mode().values[0])

category = pd.get_dummies(category)

category.shape

"""### numeric"""

numeric = features.select_dtypes(include=[int,float])

numeric.head()

from sklearn.impute import SimpleImputer

imputer = SimpleImputer(strategy='mean', missing_values=np.nan)

numeric =  pd.DataFrame(imputer.fit_transform(numeric), columns=numeric.columns, index=numeric.index)

features_clean = category.join(numeric)
features_clean.shape

"""Finally our data is ready

## Preparing our data
"""

X_train, X_test, y_train, y_test = train_test_split(features_clean, labels, test_size=0.30, random_state=202)

X_train.shape

X_test.shape

"""# Random Forest"""

from sklearn.ensemble import RandomForestRegressor

clf = RandomForestRegressor(n_estimators=20, n_jobs=-1)

clf.fit(X_train, y_train)

clf.estimators_[0:5]

"""### Visualization"""

# ! pip install graphviz

# Windows
# !set PATH=PATH;C:\path\to\anaconda\Library\bin\graphviz\

# Mac
# brew install graphviz

# Linux
# sudo apt-get install graphviz

from sklearn.tree import export_graphviz

import graphviz

dot_data = export_graphviz(clf.estimators_[0], out_file=None, 
                     feature_names=features_clean.columns,  
                     class_names=labels.unique(),  
                     filled=True, rounded=True,  
                     special_characters=True)  
graph = graphviz.Source(dot_data)  
graph

"""## Metrics"""

from sklearn.metrics import r2_score, mean_squared_error

from math import sqrt

"""### How well does 1 tree perform?"""

tree_predictions = clf.estimators_[0].predict(X_test)

r2_score(y_test, tree_predictions)

sqrt(mean_squared_error(y_test, tree_predictions))

"""### How do all our trees perform?"""

forest_preditions = clf.predict(X_test)

r2_score(y_test, forest_preditions)

sqrt(mean_squared_error(y_test, forest_preditions))

preds = np.stack([t.predict(X_test) for t in clf.estimators_])

plt.plot([r2_score(y_test, np.mean(preds[:i+1], axis=0)) for i in range(clf.n_estimators)])

"""### OOB score"""

clf = RandomForestRegressor(n_estimators=100, 
                            oob_score=True, 
                            n_jobs=-1)
clf.fit(X_train, y_train)
pred = clf.predict(X_test)
clf.oob_score_, r2_score(y_test, pred)

"""## Hyperparameters
- Max depth
- Max features
    - auto
    - sqrt
    - log2
    - int
    - float
- Min Samples split
- Min samples leaf
"""

from sklearn.ensemble import ExtraTreesRegressor

X_train.shape

"""%%time
clf = RandomForestRegressor(n_estimators=500, 
                            oob_score=True,
                            min_samples_leaf=2,
                            n_jobs=-1,
                            bootstrap=True)
clf.fit(X_train, y_train)
pred = clf.predict(X_test)

print(clf.oob_score_, r2_score(y_test, pred))

%%time
clf = ExtraTreesRegressor(n_estimators=500, 
#                             min_samples_leaf=3,
                            max_features=0.5,
                            max_depth = 7,
                            n_jobs=-1)
clf.fit(X_train, y_train)
pred = clf.predict(X_test)

print(r2_score(y_test, pred))

### Feature Importance
"""

feature_importances = pd.DataFrame(clf.feature_importances_,
                                   index = X_train.columns,
                                   columns=['importance']).sort_values('importance', ascending=False)

feature_importances.iloc[0:25].plot.barh()

feature_importances[feature_importances['importance']> feature_importances['importance'].mean()]

subset_feat = feature_importances[feature_importances['importance']> feature_importances['importance'].mean()].index

clf = RandomForestRegressor(n_estimators=500, 
                            oob_score=True,
                            min_samples_leaf=2,
                            n_jobs=-1,
                            bootstrap=True)
clf.fit(X_train[subset_feat], y_train)
pred = clf.predict(X_test[subset_feat])

print(clf.oob_score_, r2_score(y_test, pred))

