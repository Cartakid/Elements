# -*- coding: utf-8 -*-
"""Class 4: Normality tests, outliers, and missing values - master.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ozvAjJ0ACRXuFrr6ZHSbNYFhQAHkbhL9

# Import Packages
"""

import pandas as pd
import numpy as np
import seaborn as sns
from scipy import stats

"""# Load data"""

# Load King County housing data
# https://www.kaggle.com/harlfoxem/housesalesprediction
housing_df = pd.read_csv('kc_house_data.csv')

# Look at last n number of observations of the dataset
housing_df.tail()

"""# Histogram plots"""

# Create a normal sample with n observations and plot a histogram of the sample
x = np.random.normal(size=1000)
sns.distplot(x)

# Compare the original, normal sample with the histogram of square feet of living space from our housing dataframe
price_plot = sns.distplot(housing_df['sqft_living'])

# Compare the original, normal sample with the histogram of a boolean variable like waterfront from our housing dataframe
sns.distplot(housing_df['waterfront'])

# Notice why the plot appears as above
housing_df['waterfront'].value_counts()

"""# Normality tests"""

# QQ plot showing left skew and non-normality of very high priced houses (long right tail)
stats.probplot(housing_df['price'], plot=sns.mpl.pyplot)

# Shapiro-Wilk test of normality
# Code source: https://machinelearningmastery.com/a-gentle-introduction-to-normality-tests-in-python/
from scipy.stats import shapiro

# normality test
stat, p = shapiro(housing_df['price'])
print('Statistics=%.3f, p=%.3f' % (stat, p))
# interpret
alpha = 0.05
if p > alpha:
    print('Sample looks Gaussian (fail to reject H0)')
else:
    print('Sample does not look Gaussian (reject H0)')

# D’Agostino’s K^2 test of normality
# Code source: https://machinelearningmastery.com/a-gentle-introduction-to-normality-tests-in-python/
from scipy.stats import normaltest

# normality test
stat, p = normaltest(housing_df['price'])
print('Statistics=%.3f, p=%.3f' % (stat, p))
# interpret
alpha = 0.05
if p > alpha:
    print('Sample looks Gaussian (fail to reject H0)')
else:
    print('Sample does not look Gaussian (reject H0)')

# Anderson-Darling test of normality
# Code source: https://machinelearningmastery.com/a-gentle-introduction-to-normality-tests-in-python/
from scipy.stats import anderson

# normality test
result = anderson(housing_df['price'])
print('Statistic: %.3f' % result.statistic)
p = 0
for i in range(len(result.critical_values)):
    sl, cv = result.significance_level[i], result.critical_values[i]
    if result.statistic < result.critical_values[i]:
        print('%.3f: %.3f, data looks normal (fail to reject H0)' % (sl, cv))
    else:
        print('Alpha = %.3f: %.3f, data does not look normal (reject H0)' % (sl, cv))

"""# Handling missing data"""

# Load data and find missing values
# https://www.kaggle.com/mihirchate95/handling-missing-values/data
missing_df = pd.read_csv('train.csv')
missing_df.apply(lambda x: x.count(), axis=0)

# Make sure the feature shape looks correct
missing_df['MiscFeature'].shape

# Explore value counts of feature with missing values -- the rest are missing values
missing_df['MiscFeature'].value_counts()

# Categorical binning to handle missing values
def missing_to_categorical(series):
    if  series['MiscFeature'] is np.nan:
        return 'No_Misc_Features'
    else:
        return series['MiscFeature']

missing_df['MiscFeatureCategory'] = missing_df.apply(lambda x: missing_to_categorical(x),axis=1)

# Now, we've converted the missing values to another categorical level
missing_df['MiscFeatureCategory'].value_counts()

# Check the current median and how many observations currently have that value
print(missing_df['LotFrontage'].median())
missing_df['LotFrontage'].loc[missing_df['LotFrontage'] == 69.0].count()

# Data imputation using median -- the na's have been replaced with the median value
missing_df['LotFrontage'].fillna(missing_df['LotFrontage'].median(), inplace=True)
missing_df['LotFrontage'].loc[missing_df['LotFrontage'] == 69.0].count()

# Load data and find missing values
counts = missing_df.apply(lambda x: x.count(), axis=0)
counts.loc[counts.apply(lambda x: x < 1460) == True]

# knn imputation
!conda install ecos --yes
!conda install CVXcanon  
!pip install fancyimpute  
import fancyimpute
from fancyimpute import KNN    
# X is the complete data matrix
# X_incomplete has the same values as X except a subset have been replace with NaN

# Use 3 nearest rows which have a feature to fill in each row's missing features
missing_df['GarageYrBlt'] = KNN(k=3).complete(missing_df['GarageYrBlt'])
missing_df['GarageYrBlt'].mean()

