# -*- coding: utf-8 -*-
"""Penalty functions - master.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17RkB5r9eGJUQyGV_Yu8hXp3GisVf_fAC

# Import packages
"""

import pandas as pd
import numpy as np
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Ridge
from sklearn.linear_model import Lasso
from sklearn.linear_model import ElasticNet
from sklearn.preprocessing import PolynomialFeatures
import statsmodels.api as sm

"""# Import data"""

# Import data from URL and add column names
auto_df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.data', header = None
                     , na_values = '?')

auto_df.columns = [
 'symboling'
 ,'normalized-losses'
 ,'make'
 ,'fuel-type'
 ,'aspiration'
 ,'num-of-doors'
 ,'body-style'
 ,'drive-wheels'
 ,'engine-location'
 ,'wheel-base'
 ,'length'
 ,'width'
 ,'height'
 ,'curb-weight'
 ,'engine-type'
 ,'num-of-cylinders'
 ,'engine-size'
 ,'fuel-system'
 ,'bore'
 ,'stroke'
 ,'compression-ratio'
 ,'horsepower'
 ,'peak-rpm'
 ,'city-mpg'
 ,'highway-mpg'
 , 'price'
]

auto_df.tail()

"""# Data imputation"""

# Impute values for columns with NAs
auto_df['normalized-losses'].fillna(auto_df['normalized-losses'].median(), inplace=True)
auto_df['num-of-doors'].fillna(auto_df['num-of-doors'].mode(), inplace=True)
auto_df['fuel-type'].fillna(auto_df['fuel-type'].mode(), inplace=True)
auto_df['num-of-cylinders'].fillna(auto_df['num-of-cylinders'].mode(), inplace=True)
auto_df['bore'].fillna(auto_df['bore'].median(), inplace=True)
auto_df['stroke'].fillna(auto_df['stroke'].median(), inplace=True)
auto_df['horsepower'].fillna(auto_df['horsepower'].median(), inplace=True)
auto_df['peak-rpm'].fillna(auto_df['peak-rpm'].median(), inplace=True)
auto_df['price'].fillna(auto_df['price'].median(), inplace=True)

"""# Data exploration"""

# Calculate correlations between the variables. What do the black squares represent?
corr = auto_df.corr()
 
# Heatmap
sns.heatmap(corr)

"""# Modeling preparation"""

# Isolate response variable
y = pd.DataFrame(auto_df['price'])

# Drop response variable from feature dataframe
X = auto_df.drop('price', axis=1)

# Choose a subset of variables to work with
X  = X[['fuel-type', 'wheel-base', 'peak-rpm', 'city-mpg']]
# X  = X[['fuel-type','peak-rpm', 'city-mpg']]

X.head()

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

X_train.head()

# Reset each index, so we'll be able to concatenate them with our encoded variables dataframe
X_train.reset_index(inplace=True)
X_test.reset_index(inplace=True)
y_train.reset_index(inplace=True)
y_test.reset_index(inplace=True)

# Take a look at the df before transformation
X_train.head()

# See levels of fuel-type
X_train['fuel-type'].value_counts()

# Source: http://www.insightsbot.com/blog/McTKK/python-one-hot-encoding-with-scikit-learn
# Get dummies from fuel-type
from sklearn.preprocessing import LabelBinarizer
fuel_type_lb = LabelBinarizer()

# Train
fuel_type_dummies_df = fuel_type_lb.fit_transform(X_train['fuel-type'].values)

# Test
fuel_type_dummies_test_df = fuel_type_lb.fit_transform(X_test['fuel-type'].values)

# Add everything back to your main df
# Train
fuel_type_dummies_df = pd.DataFrame(fuel_type_dummies_df, columns = ["FuelType_"+str(int(i)) for i in range(fuel_type_dummies_df.shape[1])])
X_train_final = pd.concat([X_train, fuel_type_dummies_df], axis=1)

#Test
fuel_type_dummies_test_df = pd.DataFrame(fuel_type_dummies_test_df, columns = ["FuelType_"+str(int(i)) for i in range(fuel_type_dummies_test_df.shape[1])])
X_test_final = pd.concat([X_test, fuel_type_dummies_test_df], axis=1)

# Take a look after transformation
X_train_final.head()

# Drop redundant columns and index
# Train
X_train_final.drop('index', axis=1, inplace=True)
X_train_final.drop('fuel-type', axis=1, inplace=True)
y_train.drop('index', axis=1, inplace=True)

# Test
X_test_final.drop('index', axis=1, inplace=True)
X_test_final.drop('fuel-type', axis=1, inplace=True)
y_test.drop('index', axis=1, inplace=True)

# Last check that everything looks right
X_train_final.head()

"""# Linear regression with lasso - sklearn"""

# Our baseline linear regression model
lr_model = LinearRegression()
lr_model.fit(X_train_final, y_train)
lr_pred = lr_model.predict(X_test_final)
print(lr_model.score(X_test_final, y_test))
lr_model.coef_

# Lasso regression model with no penalty (OLS)
lasso_model_0 = Lasso(alpha=0, fit_intercept=True)
lasso_model_0.fit(X_train_final, y_train)
print(lasso_model_0.score(X_test_final, y_test))
lasso_model_0.coef_

# Lasso regression model with alpha = 10 (some penalty)
lasso_model_10 = Lasso(alpha=10, fit_intercept=True)
lasso_model_10.fit(X_train_final, y_train)
print(lasso_model_10.score(X_test_final, y_test))
lasso_model_10.coef_

# Lasso regression model with alpha = 10000 (high penalty)
lasso_model_10k = Lasso(alpha=10000, fit_intercept=True)
lasso_model_10k.fit(X_train_final, y_train)
lasso_10k_pred = lasso_model_10k.predict(X_test_final)
print(lasso_model_10k.score(X_test_final, y_test))
lasso_model_10k.coef_

# Plot to compare models
sns.scatterplot(x = X_test_final['city-mpg'], y = y_test.values.ravel())
sns.regplot(x = X_test_final['city-mpg'], y = lr_pred.ravel(), color='g')
sns.regplot(x = X_test_final['city-mpg'], y = lasso_10k_pred.ravel(), color='purple')

"""# Ridge regression - sklearn"""

# Ridge regression with high penalty
ridge_model = Ridge(alpha=10000, fit_intercept=True)
ridge_model.fit(X_train_final, y_train)
ridge_pred = ridge_model.predict(X_test_final)
print(ridge_model.score(X_test_final, y_test))
ridge_model.coef_

# Plot to compare models
sns.scatterplot(x = X_test_final['city-mpg'], y = y_test.values.ravel())
sns.regplot(x = X_test_final['city-mpg'], y = lr_pred.ravel(), color='g')
sns.regplot(x = X_test_final['city-mpg'], y = ridge_pred.ravel(), color='purple')

"""# Lasso polynomial regression - sklearn"""

# Create polynomial regression features of nth degree
poly_reg = PolynomialFeatures(degree = 3)
X_poly_train = poly_reg.fit_transform(pd.DataFrame(X_train_final))
X_poly_test = poly_reg.fit_transform(pd.DataFrame(X_test_final))
poly_result = poly_reg.fit(X_train_final, y_train)

# Fit linear model now polynomial features
poly_model = LinearRegression()
poly_result = poly_model.fit(X_poly_train, y_train)
y_poly_pred = poly_model.predict(X_poly_test)

# Lasso regression model with alpha = 10000 (high penalty) -- now with a lot of new variables
lasso_poly_model_10k = Lasso(alpha=10000, fit_intercept=True)
lasso_poly_model_10k.fit(X_poly_train, y_train)
lasso_poly_10k_pred = lasso_poly_model_10k.predict(X_poly_test)
print(lasso_poly_model_10k.score(X_poly_test, y_test))
lasso_poly_model_10k.coef_

# Plot to compare models
sns.scatterplot(x = X_test_final['city-mpg'], y = y_test.values.ravel())
sns.regplot(x = X_test_final['city-mpg'], y = lr_pred.ravel(), color='g')
sns.regplot(x = X_test_final['city-mpg'], y = y_poly_pred.ravel(), color='gray', order=3)
sns.regplot(x = X_test_final['city-mpg'], y = lasso_poly_10k_pred.ravel(), color='purple', order=3)

"""# Appendix: handling many levels of categorical variables"""

# See that this is a multi-class categorical variable
X_train['num-of-cylinders'].value_counts()

# Take a subset of columns
X_train_pred = X_train[['fuel-type', 'wheel-base', 'peak-rpm', 'city-mpg', 'num-of-cylinders']]

# Source: http://www.insightsbot.com/blog/McTKK/python-one-hot-encoding-with-scikit-learn
# Create dummy variables 
from sklearn.preprocessing import LabelBinarizer
fuel_type_lb = LabelBinarizer()
num_cyl_mlb = LabelBinarizer()
X = fuel_type_lb.fit_transform(X_train_pred['fuel-type'].values)
X2 = num_cyl_mlb.fit_transform(X_train_pred['num-of-cylinders'].values)

# Determine variable order and rename
col_levels = ['eight', 'five', 'four', 'six', 'twelve', 'two']

# Naming columns to make sense
dfOneHot = pd.DataFrame(X, columns = ["FuelType_"+str(int(i)) for i in range(X.shape[1])])
dfOneHot2 = pd.DataFrame(X2, columns = ["NumCyl_"+str(col_levels[i]) for i in range(X2.shape[1])])
df = pd.concat([X_train_pred, dfOneHot, dfOneHot2], axis=1)

# Drop unecessary columns
df.drop('index', axis=1, inplace=True)
df.drop('fuel-type', axis=1, inplace=True)
df.drop('num-of-cylinders', axis=1, inplace=True)

"""# Appendix: alternate categorical variable encoding - one-hot encoding"""

# Take a subset of columns
X_train_pred = X_train[['fuel-type', 'wheel-base', 'peak-rpm', 'city-mpg', 'num-of-cylinders']]

# Encode categorical variables to numerical levels
from sklearn.preprocessing import LabelEncoder
le_fuel_type = LabelEncoder()
X_train_pred['fuel-type'] = le_fuel_type.fit_transform(X_train_pred['fuel-type'])

# Get dummies from numerical levels
from sklearn.preprocessing import OneHotEncoder
fuel_type_ohe = OneHotEncoder()
X = fuel_type_ohe.fit_transform(X_train_pred['fuel-type'].values.reshape(-1,1)).toarray()

# Reset the training df index
X_train_pred.reset_index(inplace=True)

# Join the one hot encoded dataframe back to your other dataframe
dfOneHot = pd.DataFrame(X, columns = ["FuelType_"+str(int(i)) for i in range(X.shape[1])])
df = pd.concat([X_train_pred, dfOneHot], axis=1)

# Take a look
df.head()

# Drop unecessary columns
df.drop('index', axis=1, inplace=True)
df.drop('fuel-type', axis=1, inplace=True)
df.drop('num-of-cylinders', axis=1, inplace=True)