# -*- coding: utf-8 -*-
"""Class 10: SVM - master.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1v3WpI9l-kLkdxFb9TwMMvJgSOWaUIidK

# Import packages
"""

import pandas as pd
import numpy as np
from sklearn.svm import SVC
import matplotlib.pyplot as plt
from sklearn import datasets
import seaborn as sns

"""# Import data"""

# import some data to play with
iris = datasets.load_iris()
iris_df = pd.DataFrame(iris.data)
iris_df.columns = iris.feature_names

"""### Initial Exploration"""

iris_df.head()

iris_featues = iris_df.iloc[:,:2] # we only take the first two features
iris_labels = pd.Series(iris.target)

iris_featues.head()

"""We check how the classes are distributed"""

sns.distplot(iris_labels, bins=5, kde=False)

iris_labels.value_counts()

"""### Plotting our classes

We can see how Class 0 can be separated from the other classes easily and maybe with a linear function. But for Classes 1 and 2 we might need to apply a kernel trick.
"""

sns.scatterplot(
    x=iris_df['sepal length (cm)'], 
    y=iris_df['sepal width (cm)'], 
    hue=iris_labels, 
    palette=sns.color_palette('bright',3)
)

"""## Training an SVM and predicting

As with most other supervised learning algorithms, we give the SVM a set of feaatures and labels to classify the data. We can see how it isn't differentiating well between Classes 1 and 2.
"""

clf = SVC(kernel = 'linear')
clf.fit(iris_featues, iris_labels)

predictions = clf.predict(iris_featues)

results = pd.DataFrame({'Labels':iris_labels, 'Predictions':predictions})

pd.concat([iris_featues, results], axis=1, sort=False).tail(15)

"""## Visualizing the hyperparameters

### Plotting function

This plot function will show us the decision function between each class and the support vectors which define it.
"""

def plotSVC(svc, param, X, y):
    clf = svc
    clf.fit(X, y)

    plt.clf()
    plt.scatter(X.iloc[:, 0], X.iloc[:, 1], c=y, zorder=10, cmap=plt.cm.Paired,
                edgecolor='k', s=20)

    # Circle out the test data
    plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=80, facecolors='none',
                zorder=10, edgecolor='k')

    plt.axis('tight')
    x_min = X.iloc[:, 0].min()
    x_max = X.iloc[:, 0].max()
    y_min = X.iloc[:, 1].min()
    y_max = X.iloc[:, 1].max()

    XX, YY = np.mgrid[x_min:x_max:200j, y_min:y_max:200j]
    pre_z = svc.predict(np.c_[XX.ravel(), YY.ravel()])

    Z = pre_z.reshape(XX.shape)
    plt.contour(XX, YY, Z, colors=['k', 'k', 'k'],
                linestyles=['--', '-', '--'])
    
    plt.pcolormesh(XX, YY, Z , cmap=plt.cm.Paired)
    plt.title(param)
    plt.show()

"""### Different behavior per Kernel

We are taking a look at 3 different kernels:

But first, what is a Kernel? 
A Kernel is a transformation applied to all points in the data before computing the hyperplane which separates the classes. Basically you are trying to maximize the separation between classes by finding a higher dimensional space in which they can be easily separated.

- Linear kernel: Which will create a linear equation between the classes
- RBF (radial basis function): Which turns every point into a probability density function of a normal distribution.
- Polynomial kernel: Which creates a polynomial combination of features up to a definite degree.
"""

kernels = ['linear', 'rbf', 'poly']
for kernel in kernels:
    svc = SVC(kernel=kernel).fit(iris_featues, iris_labels)
    plotSVC(svc, kernel, iris_featues, iris_labels)

"""### Gamma

The gamma parameters can be seen as the inverse of the radius of influence of samples selected by the model as support vectors.
It tells our algorithm how far to look for in the data for support vectors. Higher values = support vectors further away from the margin, lower values = closer to the margin.

Increasing the gamma may lead to overfitting in our data.
"""

gammas = [0.1, 1, 10, 100]
for gamma in gammas:
    svc = SVC(kernel='rbf', gamma=gamma).fit(iris_featues, iris_labels)
    plotSVC(svc ,gamma, iris_featues, iris_labels)

"""### C

C is equal to the regularization parameter. It trades off correct classification of training examples against maximization of the decision functionâ€™s margin. 

For larger values of C, a smaller margin will be accepted if the decision function is better at classifying all training points correctly. 
A lower C will encourage a larger margin, therefore a simpler decision function, at the cost of training accuracy
"""

cs = [0.1, 1, 10, 100, 1000]
for c in cs:
    svc = SVC(kernel= 'rbf' , C=c).fit(iris_featues, iris_labels)
    plotSVC(svc, c, iris_featues, iris_labels)

"""### Polynomial Degree

Degree only applies to poly kernel. It defines the degree of the polynomial function, increasing the number of feature combinations.

It appears that the degree parameter controls the flexibility of the decision boundary. Higher degree kernels yield a more flexible decision boundary.
"""

degrees = [0, 1, 2, 3, 4, 5, 6]

for degree in degrees:
    svc = SVC(kernel='poly', degree=degree).fit(iris_featues, iris_labels)
    plotSVC(svc, degree, iris_featues, iris_labels)

"""## Create your own"""

svc = SVC(kernel='linear', degree=1, gamma=1, C=10).fit(iris_featues, iris_labels)
plotSVC(svc, 'Your own', iris_featues, iris_labels)

